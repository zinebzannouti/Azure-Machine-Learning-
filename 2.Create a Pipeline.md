# **Create a Pipeline**

![a](https://user-images.githubusercontent.com/78825764/207307468-5deee032-67bf-4877-8715-0fe6dad8a444.PNG)


  #####  Vous pouvez utiliser le SDK Azure Machine Learning pour effectuer toutes les tâches requises pour créer et exploiter une solution Machine Learning dans Azure. Plutôt que d'effectuer ces tâches individuellement, vous pouvez utiliser des pipelines pour orchestrer les étapes requises pour préparer les données, exécuter des scripts de formation, enregistrer des modèles et d'autres tâches.
  
---

- *Dans cet exercice, le code pour créer et publier un pipeline est fourni dans un notebook*

  **1.In the Notebooks page, browse to the /users/your-user-name/mslearn-dp100 folder where you cloned the notebook repository, and open the Create a Pipeline notebook.**

![a](https://user-images.githubusercontent.com/78825764/207055582-04bb3f6c-1df5-4d45-9c74-97231786e212.PNG)

 
 **2.Then read the notes in the notebook, running each code cell in turn.**
 
 ## 1-Prepare Data :
 
 - Après l'exécution de cette cellule votre data sera enregistrer dans AML studio :
 
![a](https://user-images.githubusercontent.com/78825764/207062270-9cbca2a9-fb7d-4986-8baa-22e0b4b355f9.PNG)

 
  
  - Cliquer sur cette icone de data :


  
  
![a](https://user-images.githubusercontent.com/78825764/207061689-5a2787a1-ff75-46bd-9393-6820cc4dadde.PNG)
---
![a](https://user-images.githubusercontent.com/78825764/207062961-14efc000-f796-4225-961e-b0d84b6a85a1.PNG)

## 2-Create scripts for pipeline steps

*Les pipelines consistent en une ou plusieurs étapes, qui peuvent être des scripts Python, ou des étapes spécialisées comme une étape de transfert de données qui copie les données d'un emplacement à un autre. Chaque étape peut s'exécuter dans son propre contexte de calcul. Dans cet exercice, vous allez créer un pipeline simple contenant deux étapes de script Python : une pour prétraiter certaines données d'entraînement et une autre pour utiliser les données prétraitées pour entraîner et enregistrer un modèle.*

**Vous avez trois cellules dans votre Notebook à Exécuter qui correspond à :**

**Etape 1:** créer un dossier pour les fichiers de script que nous utiliserons dans les étapes du pipeline.



**Etape 2:** créez le premier script, qui lira les données de l'ensemble de données sur le diabète et appliquera un prétraitement simple pour supprimer toutes les lignes avec des données manquantes et normaliser les caractéristiques numériques afin qu'elles soient sur une échelle similaire.


**Etape 3:** créez le script pour la deuxième étape, qui entrainera un modèle.

## 3- Prepare a compute environment for the pipeline steps

- *Dans cet exercice, vous utiliserez le même calcul pour les deux étapes, mais il est important de réaliser que chaque étape est exécutée indépendamment ; vous pouvez donc spécifier différents contextes de calcul pour chaque étape, le cas échéant.*

- *The compute will require a Python environment with the necessary package dependencies installed.*

![a](https://user-images.githubusercontent.com/78825764/207072305-3dd1567e-2cdd-4f1f-9e30-52f17507401b.PNG)

## 4- Create and run a pipeline

- *Vous êtes maintenant prêt à créer et à exécuter un pipeline.*

- *Vous devez d'abord définir les étapes du pipeline et toutes les références de données qui doivent être transmises entre elles. Dans ce cas, la première étape doit écrire les données préparées dans un dossier qui peut être lu par la deuxième étape.*
- *Étant donné que les étapes seront exécutées sur un calcul distant (et en fait, chacune pourrait être exécutée sur un calcul différent), le chemin du dossier doit être transmis en tant que référence de données à un emplacement dans un magasin de données dans l'espace de travail.*
- *L'objet OutputFileDatasetConfig est un type spécial de référence de données qui est utilisé pour les emplacements de stockage provisoires qui peuvent être transmis entre les étapes du pipeline.*
- *Vous allez donc en créer un et l'utiliser comme sortie pour la première étape et comme entrée pour la deuxième étape.* 
- *Notez que vous devez le transmettre en tant qu'argument de script afin que votre code puisse accéder à l'emplacement du magasin de données référencé par la référence de données.*

![a](https://user-images.githubusercontent.com/78825764/207136897-0060ed08-f22c-44e3-8e5b-dae03f7fbc3b.PNG)

- *Maintenant  vous êtes prêt à créer le pipeline à partir des étapes que vous avez définies et à l'exécuter en tant qu'expérience.*



![a](https://user-images.githubusercontent.com/78825764/207138072-cb47583d-8338-4b3d-b404-89e71608eb83.PNG)

- *Après la fin de l'exécution , voici le résultat de la cellule:*

![a](https://user-images.githubusercontent.com/78825764/207139352-65dccfa0-8421-4bd1-8090-b5362faea7c4.PNG)

- *Une représentation graphique de l'expérience de pipeline s'affichera dans le widget lors de son exécution. Vous pouvez également surveiller les exécutions de pipeline dans la page Expériences d'Azure Machine Learning Studio.*

- *Go to Jobs > All jobs :*

![a](https://user-images.githubusercontent.com/78825764/207140780-53b198ed-6e0f-4111-a69f-cf78dbe9e329.PNG)

![a](https://user-images.githubusercontent.com/78825764/207144713-85844f2f-cedb-46d0-ba79-f629d3125916.PNG)


- *Double cliquer sur le Job créer :*


![a](https://user-images.githubusercontent.com/78825764/207145056-34b27400-ee6b-4d2a-aa21-1501c689141a.PNG)

- *Double cliquer sur chaque étape dans cette pipeline pour visualiser les différents informations en relation avec chaque étape ( comme les metrics , paramètres , outputs , logs ...)*

![a](https://user-images.githubusercontent.com/78825764/207146024-030fe240-10a4-4243-8871-dbbae9e31e8d.PNG)


![a](https://user-images.githubusercontent.com/78825764/207146247-46982b2f-fba2-459c-bbb8-fc16b1e68d89.PNG)

- *En supposant que le pipeline a réussi, un nouveau modèle doit être enregistré avec un tag de contexte de training indiquant qu'il a été formé dans un pipeline. Exécutez le code suivant pour vérifier cela.*


![a](https://user-images.githubusercontent.com/78825764/207147027-6e0da11e-ac4a-4f9c-b467-0f30ecf65bd8.PNG)

- *Go to Models :*




![a](https://user-images.githubusercontent.com/78825764/207147726-ae0c67e4-f1f1-4aee-8296-f8162279acf2.PNG)

- *Voici le model qu'on a enregistré :*



![a](https://user-images.githubusercontent.com/78825764/207153652-621ca0da-be18-4fc3-985a-afe461eeab5d.PNG)

## 5-Publish the pipeline :

- *Après avoir créé et testé un pipeline, vous pouvez le publier en tant que service REST.*

![a](https://user-images.githubusercontent.com/78825764/207156874-dab201aa-ad44-4fb5-ad0a-a30ad74383ba.PNG)

- *Notez que le pipeline publié a un point de terminaison, que vous pouvez voir dans la page Points de terminaison (sur l'onglet Points de terminaison du pipeline) dans Azure Machine Learning Studio. Vous pouvez également trouver son URI en tant que propriété de l'objet de pipeline publié :*

- *GO to pipeline > pipeline endpoints*


![a](https://user-images.githubusercontent.com/78825764/207158245-252b4766-0f1b-4223-bdc9-9c4ba8b16068.PNG)

## 6- Call the pipeline endpoint

- *Pour utiliser le point de terminaison, les applications clientes doivent effectuer un appel REST via HTTP. Cette demande doit être authentifiée, donc un en-tête d'autorisation est requis. Une application réelle nécessiterait un principal de service avec lequel s'authentifier, mais pour le tester, nous utiliserons l'en-tête d'autorisation de votre connexion actuelle à votre espace de travail Azure, que vous pouvez obtenir à l'aide du code suivant :*

![a](https://user-images.githubusercontent.com/78825764/207159360-fb10e4fd-9a4c-4173-b26e-53564c53824c.PNG)

- *Nous sommes maintenant prêts à appeler l'interface REST. Le pipeline s'exécute de manière asynchrone, nous récupérons donc un identifiant, que nous pouvons utiliser pour suivre l'expérience du pipeline pendant son exécution :*


![a](https://user-images.githubusercontent.com/78825764/207159889-39494670-0c0d-4efb-b29f-48ef38456780.PNG)

- *Puisque vous avez l'ID d'exécution, vous pouvez l'utiliser pour attendre la fin de l'exécution.*

## 7-Schedule the Pipeline

**Cette partie est optionnel , mais elle est très importante lorsque vous travailler sur un projet où vous allez avoir à mettre à jour votre base de données .Supposons que la clinique pour les patients diabétiques collecte de nouvelles données chaque semaine et les ajoute à l'ensemble de données. Vous pouvez exécuter le pipeline chaque semaine pour recycler le modèle avec les nouvelles données.**

- *Vous pouvez exécuter le reste du notebook.*

